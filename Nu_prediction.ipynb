{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASES</th>\n",
       "      <th>A</th>\n",
       "      <th>E</th>\n",
       "      <th>Area</th>\n",
       "      <th>N</th>\n",
       "      <th>hconv</th>\n",
       "      <th>Nu</th>\n",
       "      <th>Tmax</th>\n",
       "      <th>Rt</th>\n",
       "      <th>UA</th>\n",
       "      <th>Afin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.260047</td>\n",
       "      <td>11</td>\n",
       "      <td>27.20</td>\n",
       "      <td>78.62</td>\n",
       "      <td>67.67</td>\n",
       "      <td>1.638</td>\n",
       "      <td>0.610501</td>\n",
       "      <td>0.02103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.118203</td>\n",
       "      <td>5</td>\n",
       "      <td>35.45</td>\n",
       "      <td>102.00</td>\n",
       "      <td>126.85</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.385356</td>\n",
       "      <td>0.01087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.378251</td>\n",
       "      <td>8</td>\n",
       "      <td>29.88</td>\n",
       "      <td>91.57</td>\n",
       "      <td>72.71</td>\n",
       "      <td>1.976</td>\n",
       "      <td>0.506073</td>\n",
       "      <td>0.01694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>11</td>\n",
       "      <td>10.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>162.85</td>\n",
       "      <td>4.164</td>\n",
       "      <td>0.240154</td>\n",
       "      <td>0.02370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.354610</td>\n",
       "      <td>5</td>\n",
       "      <td>29.00</td>\n",
       "      <td>88.97</td>\n",
       "      <td>94.85</td>\n",
       "      <td>2.869</td>\n",
       "      <td>0.348554</td>\n",
       "      <td>0.01200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.189125</td>\n",
       "      <td>8</td>\n",
       "      <td>31.60</td>\n",
       "      <td>96.90</td>\n",
       "      <td>61.15</td>\n",
       "      <td>1.421</td>\n",
       "      <td>0.703730</td>\n",
       "      <td>0.02220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.520095</td>\n",
       "      <td>11</td>\n",
       "      <td>20.15</td>\n",
       "      <td>61.80</td>\n",
       "      <td>63.03</td>\n",
       "      <td>1.564</td>\n",
       "      <td>0.639386</td>\n",
       "      <td>0.03170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.236407</td>\n",
       "      <td>5</td>\n",
       "      <td>30.40</td>\n",
       "      <td>93.30</td>\n",
       "      <td>77.35</td>\n",
       "      <td>2.101</td>\n",
       "      <td>0.475964</td>\n",
       "      <td>0.01560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0.567376</td>\n",
       "      <td>8</td>\n",
       "      <td>25.10</td>\n",
       "      <td>76.90</td>\n",
       "      <td>63.25</td>\n",
       "      <td>1.591</td>\n",
       "      <td>0.628536</td>\n",
       "      <td>0.02500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.189125</td>\n",
       "      <td>8</td>\n",
       "      <td>32.08</td>\n",
       "      <td>98.34</td>\n",
       "      <td>76.30</td>\n",
       "      <td>1.946</td>\n",
       "      <td>0.513875</td>\n",
       "      <td>0.01600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.236407</td>\n",
       "      <td>5</td>\n",
       "      <td>29.24</td>\n",
       "      <td>89.62</td>\n",
       "      <td>98.76</td>\n",
       "      <td>2.991</td>\n",
       "      <td>0.334336</td>\n",
       "      <td>0.01140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>11</td>\n",
       "      <td>10.27</td>\n",
       "      <td>31.47</td>\n",
       "      <td>94.70</td>\n",
       "      <td>2.896</td>\n",
       "      <td>0.345304</td>\n",
       "      <td>0.03360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CASES   A  E      Area   N  hconv      Nu    Tmax     Rt        UA  \\\n",
       "0       1  25  1  0.260047  11  27.20   78.62   67.67  1.638  0.610501   \n",
       "1       3  25  1  0.118203   5  35.45  102.00  126.85  2.595  0.385356   \n",
       "2       5  25  2  0.378251   8  29.88   91.57   72.71  1.976  0.506073   \n",
       "3       7  25  3  0.780142  11  10.00   31.00  162.85  4.164  0.240154   \n",
       "4       9  25  3  0.354610   5  29.00   88.97   94.85  2.869  0.348554   \n",
       "5      11  35  1  0.189125   8  31.60   96.90   61.15  1.421  0.703730   \n",
       "6      13  35  2  0.520095  11  20.15   61.80   63.03  1.564  0.639386   \n",
       "7      15  35  2  0.236407   5  30.40   93.30   77.35  2.101  0.475964   \n",
       "8      17  35  3  0.567376   8  25.10   76.90   63.25  1.591  0.628536   \n",
       "9       2  25  1  0.189125   8  32.08   98.34   76.30  1.946  0.513875   \n",
       "10      6  25  2  0.236407   5  29.24   89.62   98.76  2.991  0.334336   \n",
       "11     16  35  3  0.780142  11  10.27   31.47   94.70  2.896  0.345304   \n",
       "\n",
       "       Afin  \n",
       "0   0.02103  \n",
       "1   0.01087  \n",
       "2   0.01694  \n",
       "3   0.02370  \n",
       "4   0.01200  \n",
       "5   0.02220  \n",
       "6   0.03170  \n",
       "7   0.01560  \n",
       "8   0.02500  \n",
       "9   0.01600  \n",
       "10  0.01140  \n",
       "11  0.03360  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "database = pd.read_excel('database_TCC.xlsx')\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = database[['A','E','N','Nu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardscaler = StandardScaler()\n",
    "standardscaler.fit(database)\n",
    "data = standardscaler.transform(database)\n",
    "database = pd.DataFrame(data,columns=database.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(standardscaler, open('standard_scaler_Nu.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error,max_error\n",
    "\n",
    "def split_x_and_y(database,x,y):\n",
    "  dataset_x = database[x]\n",
    "  dataset_y = database[y]\n",
    "  return dataset_x,dataset_y\n",
    "\n",
    "\n",
    "def scores(Y_true, Y_predicted):\n",
    "  r2 = r2_score(Y_true, Y_predicted)\n",
    "  meansquarederror = mean_squared_error(Y_true, Y_predicted)\n",
    "  meanabsoluteerror = mean_absolute_error(Y_true, Y_predicted)\n",
    "  maxerror = max_error(Y_true, Y_predicted)\n",
    "\n",
    "  print('r2:',r2,'meansquarederror:',meansquarederror,'meanabsoluteerror:',meanabsoluteerror,'maxerror:',maxerror)\n",
    "  return r2,meansquarederror,meanabsoluteerror,maxerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9897242898141063"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor,plot_tree\n",
    "DTR = DecisionTreeRegressor(random_state=100)\n",
    "DTR.fit(dataset_x,dataset_y)\n",
    "DTR.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05821648, 0.33288972, 0.6088938 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTR.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 25. ,   1. ,   8. , 102. ],\n",
       "       [ 25. ,   2. ,   5. ,  93.3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizar = test_x.copy()\n",
    "desnormalizar['Nu'] = DTR.predict(test_x)\n",
    "desnormalizado_teste = standardscaler.inverse_transform(desnormalizar)\n",
    "desnormalizado_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.  ,  1.  ,  8.  , 98.34],\n",
       "       [25.  ,  2.  ,  5.  , 89.62],\n",
       "       [35.  ,  3.  , 11.  , 31.47]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizar = test_x.copy()\n",
    "desnormalizar['Nu'] = test_y\n",
    "desnormalizado_resultado = standardscaler.inverse_transform(desnormalizar)\n",
    "desnormalizado_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.9897242898141063 meansquarederror: 9.052966666666642 meanabsoluteerror: 2.6033333333333317 maxerror: 3.6799999999999926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9897242898141063, 9.052966666666642, 2.6033333333333317, 3.6799999999999926)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores(desnormalizado_resultado[:,-1],desnormalizado_teste[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(DTR, open('Decision_Tree_Regressor_Nu.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig = plt.figure(figsize=(25,20))\n",
    "# _ = plot_tree(DTR, \n",
    "#                    feature_names=dataset_x.columns, \n",
    "#                    filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8326467092134622"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "LR = LinearRegression()\n",
    "LR.fit(dataset_x,dataset_y)\n",
    "LR.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96034797005603"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly = PolynomialFeatures(4,interaction_only=False,include_bias=False)\n",
    "LR = LinearRegression()\n",
    "\n",
    "dataset_x_transformed = poly.fit_transform(dataset_x)\n",
    "test_x_transformed = poly.fit_transform(test_x)\n",
    "\n",
    "LR.fit(dataset_x_transformed,dataset_y)\n",
    "LR.score(test_x_transformed,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(LR, open('Linear_Regression_with_Poly_Nu.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8570327827356772"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "MLP = MLPRegressor(random_state=100)\n",
    "MLP.fit(dataset_x,dataset_y)\n",
    "MLP.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "MLP = MLPRegressor(max_iter=1000,random_state=100)\n",
    "\n",
    "param_grid = {\n",
    "    \"activation\":['logistic','tanh','relu'],\n",
    "    \"learning_rate\":['constant','invscaling','adaptive'],\n",
    "    \"momentum\":[0.7,0.8,0.9,0.95,0.98],\n",
    "    \"n_iter_no_change\":[5,8,10,12,14],\n",
    "    \"alpha\":[0.0001,0.0002,0.0004,0.0005,0.0006,0.0008]\n",
    "}\n",
    "\n",
    "random_cv = RandomizedSearchCV(\n",
    "    MLP, param_grid, n_iter=250, cv=3, n_jobs=-1, random_state = 100\n",
    ")\n",
    "\n",
    "modelo = random_cv.fit(dataset_x,dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_iter_no_change': 5,\n",
       " 'momentum': 0.98,\n",
       " 'learning_rate': 'invscaling',\n",
       " 'alpha': 0.0008,\n",
       " 'activation': 'logistic'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8340001874556842"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "MLP = MLPRegressor(**modelo.best_params_,max_iter=1000,random_state=100)\n",
    "MLP.fit(dataset_x,dataset_y)\n",
    "MLP.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables,results = split_x_and_y(database,['A','E','N'],'Nu')\n",
    "variables.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 1s 711ms/step - loss: 1.0375 - mse: 1.0375 - val_loss: 0.3022 - val_mse: 0.3022\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0276 - mse: 1.0276 - val_loss: 0.3076 - val_mse: 0.3076\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0202 - mse: 1.0202 - val_loss: 0.3153 - val_mse: 0.3153\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0156 - mse: 1.0156 - val_loss: 0.3246 - val_mse: 0.3246\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0133 - mse: 1.0133 - val_loss: 0.3345 - val_mse: 0.3345\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0130 - mse: 1.0130 - val_loss: 0.3437 - val_mse: 0.3437\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0138 - mse: 1.0138 - val_loss: 0.3510 - val_mse: 0.3510\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0149 - mse: 1.0149 - val_loss: 0.3555 - val_mse: 0.3555\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0157 - mse: 1.0157 - val_loss: 0.3573 - val_mse: 0.3573\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0159 - mse: 1.0159 - val_loss: 0.3566 - val_mse: 0.3566\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0154 - mse: 1.0154 - val_loss: 0.3538 - val_mse: 0.3538\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0144 - mse: 1.0144 - val_loss: 0.3498 - val_mse: 0.3498\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0131 - mse: 1.0131 - val_loss: 0.3449 - val_mse: 0.3449\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0118 - mse: 1.0118 - val_loss: 0.3396 - val_mse: 0.3396\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0106 - mse: 1.0106 - val_loss: 0.3345 - val_mse: 0.3345\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.0097 - mse: 1.0097 - val_loss: 0.3298 - val_mse: 0.3298\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0091 - mse: 1.0091 - val_loss: 0.3258 - val_mse: 0.3258\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0087 - mse: 1.0087 - val_loss: 0.3225 - val_mse: 0.3225\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0085 - mse: 1.0085 - val_loss: 0.3200 - val_mse: 0.3200\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0084 - mse: 1.0084 - val_loss: 0.3183 - val_mse: 0.3183\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0082 - mse: 1.0082 - val_loss: 0.3174 - val_mse: 0.3174\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0078 - mse: 1.0078 - val_loss: 0.3172 - val_mse: 0.3172\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0073 - mse: 1.0073 - val_loss: 0.3176 - val_mse: 0.3176\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0065 - mse: 1.0065 - val_loss: 0.3186 - val_mse: 0.3186\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0056 - mse: 1.0056 - val_loss: 0.3200 - val_mse: 0.3200\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0047 - mse: 1.0047 - val_loss: 0.3218 - val_mse: 0.3218\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0037 - mse: 1.0037 - val_loss: 0.3239 - val_mse: 0.3239\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0028 - mse: 1.0028 - val_loss: 0.3260 - val_mse: 0.3260\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0018 - mse: 1.0018 - val_loss: 0.3281 - val_mse: 0.3281\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0010 - mse: 1.0010 - val_loss: 0.3299 - val_mse: 0.3299\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0001 - mse: 1.0001 - val_loss: 0.3312 - val_mse: 0.3312\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9991 - mse: 0.9991 - val_loss: 0.3320 - val_mse: 0.3320\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9981 - mse: 0.9981 - val_loss: 0.3322 - val_mse: 0.3322\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9969 - mse: 0.9969 - val_loss: 0.3317 - val_mse: 0.3317\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9955 - mse: 0.9955 - val_loss: 0.3307 - val_mse: 0.3307\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9940 - mse: 0.9940 - val_loss: 0.3292 - val_mse: 0.3292\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9924 - mse: 0.9924 - val_loss: 0.3273 - val_mse: 0.3273\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.9906 - mse: 0.9906 - val_loss: 0.3253 - val_mse: 0.3253\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9886 - mse: 0.9886 - val_loss: 0.3231 - val_mse: 0.3231\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9866 - mse: 0.9866 - val_loss: 0.3210 - val_mse: 0.3210\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9844 - mse: 0.9844 - val_loss: 0.3191 - val_mse: 0.3191\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9820 - mse: 0.9820 - val_loss: 0.3174 - val_mse: 0.3174\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9794 - mse: 0.9794 - val_loss: 0.3160 - val_mse: 0.3160\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9766 - mse: 0.9766 - val_loss: 0.3149 - val_mse: 0.3149\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9734 - mse: 0.9734 - val_loss: 0.3141 - val_mse: 0.3141\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9700 - mse: 0.9700 - val_loss: 0.3135 - val_mse: 0.3135\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9662 - mse: 0.9662 - val_loss: 0.3132 - val_mse: 0.3132\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9621 - mse: 0.9621 - val_loss: 0.3129 - val_mse: 0.3129\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9576 - mse: 0.9576 - val_loss: 0.3126 - val_mse: 0.3126\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9526 - mse: 0.9526 - val_loss: 0.3122 - val_mse: 0.3122\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9473 - mse: 0.9473 - val_loss: 0.3116 - val_mse: 0.3116\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9414 - mse: 0.9414 - val_loss: 0.3106 - val_mse: 0.3106\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.9350 - mse: 0.9350 - val_loss: 0.3093 - val_mse: 0.3093\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.9279 - mse: 0.9279 - val_loss: 0.3075 - val_mse: 0.3075\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.9203 - mse: 0.9203 - val_loss: 0.3053 - val_mse: 0.3053\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.9119 - mse: 0.9119 - val_loss: 0.3028 - val_mse: 0.3028\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.9028 - mse: 0.9028 - val_loss: 0.2999 - val_mse: 0.2999\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8929 - mse: 0.8929 - val_loss: 0.2969 - val_mse: 0.2969\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8822 - mse: 0.8822 - val_loss: 0.2938 - val_mse: 0.2938\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8706 - mse: 0.8706 - val_loss: 0.2908 - val_mse: 0.2908\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.8580 - mse: 0.8580 - val_loss: 0.2879 - val_mse: 0.2879\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8445 - mse: 0.8445 - val_loss: 0.2853 - val_mse: 0.2853\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.8301 - mse: 0.8301 - val_loss: 0.2829 - val_mse: 0.2829\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.8146 - mse: 0.8146 - val_loss: 0.2810 - val_mse: 0.2810\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.7982 - mse: 0.7982 - val_loss: 0.2793 - val_mse: 0.2793\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.7808 - mse: 0.7808 - val_loss: 0.2780 - val_mse: 0.2780\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.7625 - mse: 0.7625 - val_loss: 0.2769 - val_mse: 0.2769\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.7433 - mse: 0.7433 - val_loss: 0.2762 - val_mse: 0.2762\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7234 - mse: 0.7234 - val_loss: 0.2758 - val_mse: 0.2758\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7028 - mse: 0.7028 - val_loss: 0.2758 - val_mse: 0.2758\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6817 - mse: 0.6817 - val_loss: 0.2763 - val_mse: 0.2763\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6602 - mse: 0.6602 - val_loss: 0.2777 - val_mse: 0.2777\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6384 - mse: 0.6384 - val_loss: 0.2802 - val_mse: 0.2802\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6165 - mse: 0.6165 - val_loss: 0.2841 - val_mse: 0.2841\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5946 - mse: 0.5946 - val_loss: 0.2897 - val_mse: 0.2897\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5728 - mse: 0.5728 - val_loss: 0.2971 - val_mse: 0.2971\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5513 - mse: 0.5513 - val_loss: 0.3061 - val_mse: 0.3061\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5301 - mse: 0.5301 - val_loss: 0.3166 - val_mse: 0.3166\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5094 - mse: 0.5094 - val_loss: 0.3281 - val_mse: 0.3281\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4892 - mse: 0.4892 - val_loss: 0.3401 - val_mse: 0.3401\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4696 - mse: 0.4696 - val_loss: 0.3522 - val_mse: 0.3522\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4507 - mse: 0.4507 - val_loss: 0.3642 - val_mse: 0.3642\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4324 - mse: 0.4324 - val_loss: 0.3758 - val_mse: 0.3758\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4147 - mse: 0.4147 - val_loss: 0.3871 - val_mse: 0.3871\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3977 - mse: 0.3977 - val_loss: 0.3981 - val_mse: 0.3981\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3815 - mse: 0.3815 - val_loss: 0.4087 - val_mse: 0.4087\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3659 - mse: 0.3659 - val_loss: 0.4186 - val_mse: 0.4186\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3510 - mse: 0.3510 - val_loss: 0.4276 - val_mse: 0.4276\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3368 - mse: 0.3368 - val_loss: 0.4354 - val_mse: 0.4354\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3232 - mse: 0.3232 - val_loss: 0.4417 - val_mse: 0.4417\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3103 - mse: 0.3103 - val_loss: 0.4467 - val_mse: 0.4467\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2980 - mse: 0.2980 - val_loss: 0.4507 - val_mse: 0.4507\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2863 - mse: 0.2863 - val_loss: 0.4540 - val_mse: 0.4540\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2753 - mse: 0.2753 - val_loss: 0.4569 - val_mse: 0.4569\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2647 - mse: 0.2647 - val_loss: 0.4594 - val_mse: 0.4594\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2548 - mse: 0.2548 - val_loss: 0.4615 - val_mse: 0.4615\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2453 - mse: 0.2453 - val_loss: 0.4629 - val_mse: 0.4629\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2364 - mse: 0.2364 - val_loss: 0.4636 - val_mse: 0.4636\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2279 - mse: 0.2279 - val_loss: 0.4637 - val_mse: 0.4637\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2199 - mse: 0.2199 - val_loss: 0.4637 - val_mse: 0.4637\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2124 - mse: 0.2124 - val_loss: 0.4637 - val_mse: 0.4637\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2052 - mse: 0.2052 - val_loss: 0.4638 - val_mse: 0.4638\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1984 - mse: 0.1984 - val_loss: 0.4637 - val_mse: 0.4637\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1919 - mse: 0.1919 - val_loss: 0.4632 - val_mse: 0.4632\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1858 - mse: 0.1858 - val_loss: 0.4622 - val_mse: 0.4622\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1800 - mse: 0.1800 - val_loss: 0.4608 - val_mse: 0.4608\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1744 - mse: 0.1744 - val_loss: 0.4593 - val_mse: 0.4593\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1691 - mse: 0.1691 - val_loss: 0.4579 - val_mse: 0.4579\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1640 - mse: 0.1640 - val_loss: 0.4565 - val_mse: 0.4565\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1592 - mse: 0.1592 - val_loss: 0.4547 - val_mse: 0.4547\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1545 - mse: 0.1545 - val_loss: 0.4525 - val_mse: 0.4525\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1501 - mse: 0.1501 - val_loss: 0.4499 - val_mse: 0.4499\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1458 - mse: 0.1458 - val_loss: 0.4472 - val_mse: 0.4472\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1417 - mse: 0.1417 - val_loss: 0.4446 - val_mse: 0.4446\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1377 - mse: 0.1377 - val_loss: 0.4420 - val_mse: 0.4420\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1340 - mse: 0.1340 - val_loss: 0.4393 - val_mse: 0.4393\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1303 - mse: 0.1303 - val_loss: 0.4363 - val_mse: 0.4363\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1268 - mse: 0.1268 - val_loss: 0.4332 - val_mse: 0.4332\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1234 - mse: 0.1234 - val_loss: 0.4300 - val_mse: 0.4300\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1202 - mse: 0.1202 - val_loss: 0.4271 - val_mse: 0.4271\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1170 - mse: 0.1170 - val_loss: 0.4242 - val_mse: 0.4242\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1140 - mse: 0.1140 - val_loss: 0.4214 - val_mse: 0.4214\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1111 - mse: 0.1111 - val_loss: 0.4184 - val_mse: 0.4184\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1083 - mse: 0.1083 - val_loss: 0.4155 - val_mse: 0.4155\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1056 - mse: 0.1056 - val_loss: 0.4126 - val_mse: 0.4126\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1029 - mse: 0.1029 - val_loss: 0.4099 - val_mse: 0.4099\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1004 - mse: 0.1004 - val_loss: 0.4074 - val_mse: 0.4074\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.4050 - val_mse: 0.4050\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0956 - mse: 0.0956 - val_loss: 0.4024 - val_mse: 0.4024\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0933 - mse: 0.0933 - val_loss: 0.3999 - val_mse: 0.3999\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0910 - mse: 0.0910 - val_loss: 0.3976 - val_mse: 0.3976\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0888 - mse: 0.0888 - val_loss: 0.3954 - val_mse: 0.3954\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0867 - mse: 0.0867 - val_loss: 0.3933 - val_mse: 0.3933\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0847 - mse: 0.0847 - val_loss: 0.3912 - val_mse: 0.3912\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0827 - mse: 0.0827 - val_loss: 0.3891 - val_mse: 0.3891\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0808 - mse: 0.0808 - val_loss: 0.3870 - val_mse: 0.3870\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0789 - mse: 0.0789 - val_loss: 0.3850 - val_mse: 0.3850\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0771 - mse: 0.0771 - val_loss: 0.3831 - val_mse: 0.3831\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0753 - mse: 0.0753 - val_loss: 0.3812 - val_mse: 0.3812\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0736 - mse: 0.0736 - val_loss: 0.3791 - val_mse: 0.3791\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0719 - mse: 0.0719 - val_loss: 0.3771 - val_mse: 0.3771\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0703 - mse: 0.0703 - val_loss: 0.3751 - val_mse: 0.3751\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0688 - mse: 0.0688 - val_loss: 0.3732 - val_mse: 0.3732\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0672 - mse: 0.0672 - val_loss: 0.3712 - val_mse: 0.3712\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0657 - mse: 0.0657 - val_loss: 0.3691 - val_mse: 0.3691\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0643 - mse: 0.0643 - val_loss: 0.3669 - val_mse: 0.3669\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0629 - mse: 0.0629 - val_loss: 0.3648 - val_mse: 0.3648\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0615 - mse: 0.0615 - val_loss: 0.3627 - val_mse: 0.3627\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0602 - mse: 0.0602 - val_loss: 0.3604 - val_mse: 0.3604\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0589 - mse: 0.0589 - val_loss: 0.3581 - val_mse: 0.3581\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0577 - mse: 0.0577 - val_loss: 0.3557 - val_mse: 0.3557\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0565 - mse: 0.0565 - val_loss: 0.3533 - val_mse: 0.3533\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0553 - mse: 0.0553 - val_loss: 0.3509 - val_mse: 0.3509\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0541 - mse: 0.0541 - val_loss: 0.3484 - val_mse: 0.3484\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0530 - mse: 0.0530 - val_loss: 0.3458 - val_mse: 0.3458\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0519 - mse: 0.0519 - val_loss: 0.3431 - val_mse: 0.3431\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0509 - mse: 0.0509 - val_loss: 0.3405 - val_mse: 0.3405\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0498 - mse: 0.0498 - val_loss: 0.3379 - val_mse: 0.3379\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0488 - mse: 0.0488 - val_loss: 0.3351 - val_mse: 0.3351\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0479 - mse: 0.0479 - val_loss: 0.3324 - val_mse: 0.3324\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0469 - mse: 0.0469 - val_loss: 0.3296 - val_mse: 0.3296\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0460 - mse: 0.0460 - val_loss: 0.3269 - val_mse: 0.3269\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0451 - mse: 0.0451 - val_loss: 0.3241 - val_mse: 0.3241\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0442 - mse: 0.0442 - val_loss: 0.3212 - val_mse: 0.3212\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.3184 - val_mse: 0.3184\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.3156 - val_mse: 0.3156\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.3128 - val_mse: 0.3128\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.3099 - val_mse: 0.3099\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.3071 - val_mse: 0.3071\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.3043 - val_mse: 0.3043\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.3015 - val_mse: 0.3015\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.2986 - val_mse: 0.2986\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.2958 - val_mse: 0.2958\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.2930 - val_mse: 0.2930\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.2902 - val_mse: 0.2902\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.2873 - val_mse: 0.2873\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.2845 - val_mse: 0.2845\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.2816 - val_mse: 0.2816\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.2788 - val_mse: 0.2788\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.2759 - val_mse: 0.2759\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0324 - mse: 0.0324 - val_loss: 0.2731 - val_mse: 0.2731\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.2702 - val_mse: 0.2702\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.2673 - val_mse: 0.2673\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0308 - mse: 0.0308 - val_loss: 0.2644 - val_mse: 0.2644\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.2615 - val_mse: 0.2615\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.2586 - val_mse: 0.2586\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.2556 - val_mse: 0.2556\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0289 - mse: 0.0289 - val_loss: 0.2527 - val_mse: 0.2527\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.2497 - val_mse: 0.2497\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.2467 - val_mse: 0.2467\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0276 - mse: 0.0276 - val_loss: 0.2437 - val_mse: 0.2437\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.2407 - val_mse: 0.2407\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.2377 - val_mse: 0.2377\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.2346 - val_mse: 0.2346\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.2316 - val_mse: 0.2316\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.2285 - val_mse: 0.2285\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.2254 - val_mse: 0.2254\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.2224 - val_mse: 0.2224\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.2193 - val_mse: 0.2193\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0242 - mse: 0.0242 - val_loss: 0.2162 - val_mse: 0.2162\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0238 - mse: 0.0238 - val_loss: 0.2131 - val_mse: 0.2131\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.2100 - val_mse: 0.2100\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.2069 - val_mse: 0.2069\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.2038 - val_mse: 0.2038\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.2007 - val_mse: 0.2007\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.1976 - val_mse: 0.1976\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.1945 - val_mse: 0.1945\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.1914 - val_mse: 0.1914\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.1883 - val_mse: 0.1883\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.1852 - val_mse: 0.1852\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.1821 - val_mse: 0.1821\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.1790 - val_mse: 0.1790\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.1759 - val_mse: 0.1759\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.1728 - val_mse: 0.1728\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.1697 - val_mse: 0.1697\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0197 - mse: 0.0197 - val_loss: 0.1666 - val_mse: 0.1666\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.1636 - val_mse: 0.1636\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.1605 - val_mse: 0.1605\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.1574 - val_mse: 0.1574\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.1544 - val_mse: 0.1544\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.1513 - val_mse: 0.1513\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.1483 - val_mse: 0.1483\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.1452 - val_mse: 0.1452\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.1422 - val_mse: 0.1422\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.1392 - val_mse: 0.1392\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.1362 - val_mse: 0.1362\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.1332 - val_mse: 0.1332\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.1302 - val_mse: 0.1302\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.1272 - val_mse: 0.1272\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.1243 - val_mse: 0.1243\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0169 - mse: 0.0169 - val_loss: 0.1213 - val_mse: 0.1213\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.1184 - val_mse: 0.1184\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.1155 - val_mse: 0.1155\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.1126 - val_mse: 0.1126\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.1097 - val_mse: 0.1097\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.1069 - val_mse: 0.1069\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.1041 - val_mse: 0.1041\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.1013 - val_mse: 0.1013\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0985 - val_mse: 0.0985\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0930 - val_mse: 0.0930\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.0903 - val_mse: 0.0903\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0876 - val_mse: 0.0876\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.0850 - val_mse: 0.0850\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.0824 - val_mse: 0.0824\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0798 - val_mse: 0.0798\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0773 - val_mse: 0.0773\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.0748 - val_mse: 0.0748\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.0723 - val_mse: 0.0723\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.0699 - val_mse: 0.0699\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.0675 - val_mse: 0.0675\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0652 - val_mse: 0.0652\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.0629 - val_mse: 0.0629\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.0606 - val_mse: 0.0606\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0584 - val_mse: 0.0584\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0562 - val_mse: 0.0562\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.0541 - val_mse: 0.0541\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0520 - val_mse: 0.0520\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.0500 - val_mse: 0.0500\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0480 - val_mse: 0.0480\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.0461 - val_mse: 0.0461\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.0442 - val_mse: 0.0442\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0424 - val_mse: 0.0424\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0407 - val_mse: 0.0407\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0133 - mse: 0.0133 - val_loss: 0.0390 - val_mse: 0.0390\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0373 - val_mse: 0.0373\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0358 - val_mse: 0.0358\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0343 - val_mse: 0.0343\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0328 - val_mse: 0.0328\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0315 - val_mse: 0.0315\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0290 - val_mse: 0.0290\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.0278 - val_mse: 0.0278\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0267 - val_mse: 0.0267\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.0248 - val_mse: 0.0248\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0240 - val_mse: 0.0240\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.0232 - val_mse: 0.0232\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0226 - val_mse: 0.0226\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0220 - val_mse: 0.0220\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0215 - val_mse: 0.0215\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0208 - val_mse: 0.0208\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0206 - val_mse: 0.0206\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0205 - val_mse: 0.0205\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0205 - val_mse: 0.0205\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0206 - val_mse: 0.0206\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0207 - val_mse: 0.0207\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.0210 - val_mse: 0.0210\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0214 - val_mse: 0.0214\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0219 - val_mse: 0.0219\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0226 - val_mse: 0.0226\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0233 - val_mse: 0.0233\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0241 - val_mse: 0.0241\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0262 - val_mse: 0.0262\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0287 - val_mse: 0.0287\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0302 - val_mse: 0.0302\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.0317 - val_mse: 0.0317\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.python.keras.layers import Dense\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "variables,results = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Dense(64, activation='sigmoid', input_shape=[len(variables.keys())]),\n",
    "  # layers.Dropout(0.1),\n",
    "  layers.Dense(128, activation='sigmoid'),\n",
    "  layers.Dense(64, activation='sigmoid'),\n",
    "  layers.Dense(16, activation='sigmoid'),\n",
    "  layers.Dense(8, activation='sigmoid'),\n",
    "  # layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "# optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mse'])\n",
    "\n",
    "history = model.fit(variables.values,results.values,epochs=300,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bc5c153fa0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss','val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 74ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.83856833],\n",
       "       [ 0.8437473 ],\n",
       "       [-1.8794724 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess = model.predict(test_x.values)\n",
    "guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>E</th>\n",
       "      <th>N</th>\n",
       "      <th>Nu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.845154</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.761644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.845154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0.773004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.183216</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-1.022023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A         E         N        Nu\n",
       "9  -0.845154 -1.224745  0.000000  0.761644\n",
       "10 -0.845154  0.000000 -1.224745  0.773004\n",
       "11  1.183216  1.224745  1.224745 -1.022023"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizar = test_x.copy()\n",
    "desnormalizar['Nu'] = guess\n",
    "desnormalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.        ,  1.        ,  8.        , 96.31316031],\n",
       "       [25.        ,  2.        ,  5.        , 96.58072486],\n",
       "       [35.        ,  3.        , 11.        , 54.30246256]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizado_teste = standardscaler.inverse_transform(desnormalizar)\n",
    "desnormalizado_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>E</th>\n",
       "      <th>N</th>\n",
       "      <th>Nu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.845154</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.845154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0.477469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.183216</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-1.991431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A         E         N        Nu\n",
       "9  -0.845154 -1.224745  0.000000  0.847698\n",
       "10 -0.845154  0.000000 -1.224745  0.477469\n",
       "11  1.183216  1.224745  1.224745 -1.991431"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizar = test_x.copy()\n",
    "desnormalizar['Nu'] = test_y\n",
    "desnormalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.  ,  1.  ,  8.  , 98.34],\n",
       "       [25.  ,  2.  ,  5.  , 89.62],\n",
       "       [35.  ,  3.  , 11.  , 31.47]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizado_resultado = standardscaler.inverse_transform(desnormalizar)\n",
    "desnormalizado_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.9792734994843473 meansquarederror: 18.260180064482245 meanabsoluteerror: 3.4292649169595086 maxerror: 7.015026464920979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9792734994843473, 18.260180064482245, 3.4292649169595086, 7.015026464920979)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores(desnormalizado_resultado[:,-1],desnormalizado_teste[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"keras_model_Nu.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edbada794071f9c875b2bc5e0e869a1d746a19a0ba8801f10239845106c51c5a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
