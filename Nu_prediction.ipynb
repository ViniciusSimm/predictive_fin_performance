{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASES</th>\n",
       "      <th>A</th>\n",
       "      <th>E</th>\n",
       "      <th>Area</th>\n",
       "      <th>N</th>\n",
       "      <th>hconv</th>\n",
       "      <th>Nu</th>\n",
       "      <th>Tmax</th>\n",
       "      <th>Rt</th>\n",
       "      <th>UA</th>\n",
       "      <th>Afin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.260047</td>\n",
       "      <td>11</td>\n",
       "      <td>27.20</td>\n",
       "      <td>78.62</td>\n",
       "      <td>67.67</td>\n",
       "      <td>1.638</td>\n",
       "      <td>0.610501</td>\n",
       "      <td>0.02103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.118203</td>\n",
       "      <td>5</td>\n",
       "      <td>35.45</td>\n",
       "      <td>102.00</td>\n",
       "      <td>126.85</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.385356</td>\n",
       "      <td>0.01087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.378251</td>\n",
       "      <td>8</td>\n",
       "      <td>29.88</td>\n",
       "      <td>91.57</td>\n",
       "      <td>72.71</td>\n",
       "      <td>1.976</td>\n",
       "      <td>0.506073</td>\n",
       "      <td>0.01694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>11</td>\n",
       "      <td>10.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>162.85</td>\n",
       "      <td>4.164</td>\n",
       "      <td>0.240154</td>\n",
       "      <td>0.02370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.354610</td>\n",
       "      <td>5</td>\n",
       "      <td>29.00</td>\n",
       "      <td>88.97</td>\n",
       "      <td>94.85</td>\n",
       "      <td>2.869</td>\n",
       "      <td>0.348554</td>\n",
       "      <td>0.01200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.189125</td>\n",
       "      <td>8</td>\n",
       "      <td>31.60</td>\n",
       "      <td>96.90</td>\n",
       "      <td>61.15</td>\n",
       "      <td>1.421</td>\n",
       "      <td>0.703730</td>\n",
       "      <td>0.02220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.520095</td>\n",
       "      <td>11</td>\n",
       "      <td>20.15</td>\n",
       "      <td>61.80</td>\n",
       "      <td>63.03</td>\n",
       "      <td>1.564</td>\n",
       "      <td>0.639386</td>\n",
       "      <td>0.03170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.236407</td>\n",
       "      <td>5</td>\n",
       "      <td>30.40</td>\n",
       "      <td>93.30</td>\n",
       "      <td>77.35</td>\n",
       "      <td>2.101</td>\n",
       "      <td>0.475964</td>\n",
       "      <td>0.01560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0.567376</td>\n",
       "      <td>8</td>\n",
       "      <td>25.10</td>\n",
       "      <td>76.90</td>\n",
       "      <td>63.25</td>\n",
       "      <td>1.591</td>\n",
       "      <td>0.628536</td>\n",
       "      <td>0.02500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.189125</td>\n",
       "      <td>8</td>\n",
       "      <td>32.08</td>\n",
       "      <td>98.34</td>\n",
       "      <td>76.30</td>\n",
       "      <td>1.946</td>\n",
       "      <td>0.513875</td>\n",
       "      <td>0.01600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.236407</td>\n",
       "      <td>5</td>\n",
       "      <td>29.24</td>\n",
       "      <td>89.62</td>\n",
       "      <td>98.76</td>\n",
       "      <td>2.991</td>\n",
       "      <td>0.334336</td>\n",
       "      <td>0.01140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>11</td>\n",
       "      <td>10.27</td>\n",
       "      <td>31.47</td>\n",
       "      <td>94.70</td>\n",
       "      <td>2.896</td>\n",
       "      <td>0.345304</td>\n",
       "      <td>0.03360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CASES   A  E      Area   N  hconv      Nu    Tmax     Rt        UA  \\\n",
       "0       1  25  1  0.260047  11  27.20   78.62   67.67  1.638  0.610501   \n",
       "1       3  25  1  0.118203   5  35.45  102.00  126.85  2.595  0.385356   \n",
       "2       5  25  2  0.378251   8  29.88   91.57   72.71  1.976  0.506073   \n",
       "3       7  25  3  0.780142  11  10.00   31.00  162.85  4.164  0.240154   \n",
       "4       9  25  3  0.354610   5  29.00   88.97   94.85  2.869  0.348554   \n",
       "5      11  35  1  0.189125   8  31.60   96.90   61.15  1.421  0.703730   \n",
       "6      13  35  2  0.520095  11  20.15   61.80   63.03  1.564  0.639386   \n",
       "7      15  35  2  0.236407   5  30.40   93.30   77.35  2.101  0.475964   \n",
       "8      17  35  3  0.567376   8  25.10   76.90   63.25  1.591  0.628536   \n",
       "9       2  25  1  0.189125   8  32.08   98.34   76.30  1.946  0.513875   \n",
       "10      6  25  2  0.236407   5  29.24   89.62   98.76  2.991  0.334336   \n",
       "11     16  35  3  0.780142  11  10.27   31.47   94.70  2.896  0.345304   \n",
       "\n",
       "       Afin  \n",
       "0   0.02103  \n",
       "1   0.01087  \n",
       "2   0.01694  \n",
       "3   0.02370  \n",
       "4   0.01200  \n",
       "5   0.02220  \n",
       "6   0.03170  \n",
       "7   0.01560  \n",
       "8   0.02500  \n",
       "9   0.01600  \n",
       "10  0.01140  \n",
       "11  0.03360  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "database = pd.read_excel('database_TCC.xlsx')\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = database[['A','E','N','Nu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardscaler = StandardScaler()\n",
    "standardscaler.fit(database)\n",
    "data = standardscaler.transform(database)\n",
    "database = pd.DataFrame(data,columns=database.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(standardscaler, open('standard_scaler_Nu.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error,max_error\n",
    "\n",
    "def split_x_and_y(database,x,y):\n",
    "  dataset_x = database[x]\n",
    "  dataset_y = database[y]\n",
    "  return dataset_x,dataset_y\n",
    "\n",
    "\n",
    "def scores(Y_true, Y_predicted):\n",
    "  r2 = r2_score(Y_true, Y_predicted)\n",
    "  meansquarederror = mean_squared_error(Y_true, Y_predicted)\n",
    "  meanabsoluteerror = mean_absolute_error(Y_true, Y_predicted)\n",
    "  maxerror = max_error(Y_true, Y_predicted)\n",
    "\n",
    "  print('r2:',r2,'meansquarederror:',meansquarederror,'meanabsoluteerror:',meanabsoluteerror,'maxerror:',maxerror)\n",
    "  return r2,meansquarederror,meanabsoluteerror,maxerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9897242898141063"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor,plot_tree\n",
    "DTR = DecisionTreeRegressor(random_state=100)\n",
    "DTR.fit(dataset_x,dataset_y)\n",
    "DTR.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05821648, 0.33288972, 0.6088938 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTR.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 25. ,   1. ,   8. , 102. ],\n",
       "       [ 25. ,   2. ,   5. ,  93.3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizar = test_x.copy()\n",
    "desnormalizar['Nu'] = DTR.predict(test_x)\n",
    "desnormalizado_teste = standardscaler.inverse_transform(desnormalizar)\n",
    "desnormalizado_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.  ,  1.  ,  8.  , 98.34],\n",
       "       [25.  ,  2.  ,  5.  , 89.62],\n",
       "       [35.  ,  3.  , 11.  , 31.47]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizar = test_x.copy()\n",
    "desnormalizar['Nu'] = test_y\n",
    "desnormalizado_resultado = standardscaler.inverse_transform(desnormalizar)\n",
    "desnormalizado_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.9897242898141063 meansquarederror: 9.052966666666642 meanabsoluteerror: 2.6033333333333317 maxerror: 3.6799999999999926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9897242898141063, 9.052966666666642, 2.6033333333333317, 3.6799999999999926)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores(desnormalizado_resultado[:,-1],desnormalizado_teste[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(DTR, open('Decision_Tree_Regressor_Nu.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig = plt.figure(figsize=(25,20))\n",
    "# _ = plot_tree(DTR, \n",
    "#                    feature_names=dataset_x.columns, \n",
    "#                    filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8326467092134622"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "LR = LinearRegression()\n",
    "LR.fit(dataset_x,dataset_y)\n",
    "LR.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96034797005603"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly = PolynomialFeatures(4,interaction_only=False,include_bias=False)\n",
    "LR = LinearRegression()\n",
    "\n",
    "dataset_x_transformed = poly.fit_transform(dataset_x)\n",
    "test_x_transformed = poly.fit_transform(test_x)\n",
    "\n",
    "LR.fit(dataset_x_transformed,dataset_y)\n",
    "LR.score(test_x_transformed,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(LR, open('Linear_Regression_with_Poly_Nu.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8570327827356772"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "MLP = MLPRegressor(random_state=100)\n",
    "MLP.fit(dataset_x,dataset_y)\n",
    "MLP.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "MLP = MLPRegressor(max_iter=1000,random_state=100)\n",
    "\n",
    "param_grid = {\n",
    "    \"activation\":['logistic','tanh','relu'],\n",
    "    \"learning_rate\":['constant','invscaling','adaptive'],\n",
    "    \"momentum\":[0.7,0.8,0.9,0.95,0.98],\n",
    "    \"n_iter_no_change\":[5,8,10,12,14],\n",
    "    \"alpha\":[0.0001,0.0002,0.0004,0.0005,0.0006,0.0008]\n",
    "}\n",
    "\n",
    "random_cv = RandomizedSearchCV(\n",
    "    MLP, param_grid, n_iter=250, cv=3, n_jobs=-1, random_state = 100\n",
    ")\n",
    "\n",
    "modelo = random_cv.fit(dataset_x,dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_iter_no_change': 5,\n",
       " 'momentum': 0.98,\n",
       " 'learning_rate': 'invscaling',\n",
       " 'alpha': 0.0008,\n",
       " 'activation': 'logistic'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8340001874556842"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "MLP = MLPRegressor(**modelo.best_params_,max_iter=1000,random_state=100)\n",
    "MLP.fit(dataset_x,dataset_y)\n",
    "MLP.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables,results = split_x_and_y(database,['A','E','N'],'Nu')\n",
    "variables.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/220\n",
      "1/1 [==============================] - 0s 487ms/step - loss: 1.1199 - mse: 1.1199 - val_loss: 0.6031 - val_mse: 0.6031\n",
      "Epoch 2/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1308 - mse: 1.1308 - val_loss: 0.5408 - val_mse: 0.5408\n",
      "Epoch 3/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0830 - mse: 1.0830 - val_loss: 0.3401 - val_mse: 0.3401\n",
      "Epoch 4/220\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9769 - mse: 0.9769 - val_loss: 0.2873 - val_mse: 0.2873\n",
      "Epoch 5/220\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0110 - mse: 1.0110 - val_loss: 0.2921 - val_mse: 0.2921\n",
      "Epoch 6/220\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0360 - mse: 1.0360 - val_loss: 0.2791 - val_mse: 0.2791\n",
      "Epoch 7/220\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9817 - mse: 0.9817 - val_loss: 0.3042 - val_mse: 0.3042\n",
      "Epoch 8/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9362 - mse: 0.9362 - val_loss: 0.3796 - val_mse: 0.3796\n",
      "Epoch 9/220\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9457 - mse: 0.9457 - val_loss: 0.4281 - val_mse: 0.4281\n",
      "Epoch 10/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9602 - mse: 0.9602 - val_loss: 0.3988 - val_mse: 0.3988\n",
      "Epoch 11/220\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9359 - mse: 0.9359 - val_loss: 0.3280 - val_mse: 0.3280\n",
      "Epoch 12/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8976 - mse: 0.8976 - val_loss: 0.2736 - val_mse: 0.2736\n",
      "Epoch 13/220\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8835 - mse: 0.8835 - val_loss: 0.2526 - val_mse: 0.2526\n",
      "Epoch 14/220\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8898 - mse: 0.8898 - val_loss: 0.2467 - val_mse: 0.2467\n",
      "Epoch 15/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8840 - mse: 0.8840 - val_loss: 0.2477 - val_mse: 0.2477\n",
      "Epoch 16/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8578 - mse: 0.8578 - val_loss: 0.2664 - val_mse: 0.2664\n",
      "Epoch 17/220\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8329 - mse: 0.8329 - val_loss: 0.3024 - val_mse: 0.3024\n",
      "Epoch 18/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8248 - mse: 0.8248 - val_loss: 0.3297 - val_mse: 0.3297\n",
      "Epoch 19/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8224 - mse: 0.8224 - val_loss: 0.3234 - val_mse: 0.3234\n",
      "Epoch 20/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8083 - mse: 0.8083 - val_loss: 0.2881 - val_mse: 0.2881\n",
      "Epoch 21/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7848 - mse: 0.7848 - val_loss: 0.2490 - val_mse: 0.2490\n",
      "Epoch 22/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7667 - mse: 0.7667 - val_loss: 0.2239 - val_mse: 0.2239\n",
      "Epoch 23/220\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.7585 - mse: 0.7585 - val_loss: 0.2131 - val_mse: 0.2131\n",
      "Epoch 24/220\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.7492 - mse: 0.7492 - val_loss: 0.2128 - val_mse: 0.2128\n",
      "Epoch 25/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7310 - mse: 0.7310 - val_loss: 0.2243 - val_mse: 0.2243\n",
      "Epoch 26/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7103 - mse: 0.7103 - val_loss: 0.2457 - val_mse: 0.2457\n",
      "Epoch 27/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6957 - mse: 0.6957 - val_loss: 0.2631 - val_mse: 0.2631\n",
      "Epoch 28/220\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6851 - mse: 0.6851 - val_loss: 0.2610 - val_mse: 0.2610\n",
      "Epoch 29/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6703 - mse: 0.6703 - val_loss: 0.2391 - val_mse: 0.2391\n",
      "Epoch 30/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6506 - mse: 0.6506 - val_loss: 0.2109 - val_mse: 0.2109\n",
      "Epoch 31/220\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6328 - mse: 0.6328 - val_loss: 0.1897 - val_mse: 0.1897\n",
      "Epoch 32/220\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6193 - mse: 0.6193 - val_loss: 0.1798 - val_mse: 0.1798\n",
      "Epoch 33/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6051 - mse: 0.6051 - val_loss: 0.1807 - val_mse: 0.1807\n",
      "Epoch 34/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5870 - mse: 0.5870 - val_loss: 0.1918 - val_mse: 0.1918\n",
      "Epoch 35/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5683 - mse: 0.5683 - val_loss: 0.2072 - val_mse: 0.2072\n",
      "Epoch 36/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5527 - mse: 0.5527 - val_loss: 0.2154 - val_mse: 0.2154\n",
      "Epoch 37/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5380 - mse: 0.5380 - val_loss: 0.2080 - val_mse: 0.2080\n",
      "Epoch 38/220\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5208 - mse: 0.5208 - val_loss: 0.1887 - val_mse: 0.1887\n",
      "Epoch 39/220\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5025 - mse: 0.5025 - val_loss: 0.1683 - val_mse: 0.1683\n",
      "Epoch 40/220\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4863 - mse: 0.4863 - val_loss: 0.1556 - val_mse: 0.1556\n",
      "Epoch 41/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4713 - mse: 0.4713 - val_loss: 0.1534 - val_mse: 0.1534\n",
      "Epoch 42/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4549 - mse: 0.4549 - val_loss: 0.1610 - val_mse: 0.1610\n",
      "Epoch 43/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4376 - mse: 0.4376 - val_loss: 0.1737 - val_mse: 0.1737\n",
      "Epoch 44/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4218 - mse: 0.4218 - val_loss: 0.1822 - val_mse: 0.1822\n",
      "Epoch 45/220\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4072 - mse: 0.4072 - val_loss: 0.1789 - val_mse: 0.1789\n",
      "Epoch 46/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3919 - mse: 0.3919 - val_loss: 0.1652 - val_mse: 0.1652\n",
      "Epoch 47/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3762 - mse: 0.3762 - val_loss: 0.1499 - val_mse: 0.1499\n",
      "Epoch 48/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3619 - mse: 0.3619 - val_loss: 0.1409 - val_mse: 0.1409\n",
      "Epoch 49/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3486 - mse: 0.3486 - val_loss: 0.1416 - val_mse: 0.1416\n",
      "Epoch 50/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3349 - mse: 0.3349 - val_loss: 0.1504 - val_mse: 0.1504\n",
      "Epoch 51/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3214 - mse: 0.3214 - val_loss: 0.1614 - val_mse: 0.1614\n",
      "Epoch 52/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3092 - mse: 0.3092 - val_loss: 0.1662 - val_mse: 0.1662\n",
      "Epoch 53/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2978 - mse: 0.2978 - val_loss: 0.1608 - val_mse: 0.1608\n",
      "Epoch 54/220\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2863 - mse: 0.2863 - val_loss: 0.1493 - val_mse: 0.1493\n",
      "Epoch 55/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2755 - mse: 0.2755 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "Epoch 56/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2659 - mse: 0.2659 - val_loss: 0.1376 - val_mse: 0.1376\n",
      "Epoch 57/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2567 - mse: 0.2567 - val_loss: 0.1429 - val_mse: 0.1429\n",
      "Epoch 58/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2477 - mse: 0.2477 - val_loss: 0.1514 - val_mse: 0.1514\n",
      "Epoch 59/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.1560 - val_mse: 0.1560\n",
      "Epoch 60/220\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2323 - mse: 0.2323 - val_loss: 0.1527 - val_mse: 0.1527\n",
      "Epoch 61/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2253 - mse: 0.2253 - val_loss: 0.1440 - val_mse: 0.1440\n",
      "Epoch 62/220\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2187 - mse: 0.2187 - val_loss: 0.1361 - val_mse: 0.1361\n",
      "Epoch 63/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2130 - mse: 0.2130 - val_loss: 0.1336 - val_mse: 0.1336\n",
      "Epoch 64/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2076 - mse: 0.2076 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 65/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2025 - mse: 0.2025 - val_loss: 0.1410 - val_mse: 0.1410\n",
      "Epoch 66/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1980 - mse: 0.1980 - val_loss: 0.1423 - val_mse: 0.1423\n",
      "Epoch 67/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1939 - mse: 0.1939 - val_loss: 0.1381 - val_mse: 0.1381\n",
      "Epoch 68/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1900 - mse: 0.1900 - val_loss: 0.1313 - val_mse: 0.1313\n",
      "Epoch 69/220\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1864 - mse: 0.1864 - val_loss: 0.1262 - val_mse: 0.1262\n",
      "Epoch 70/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1833 - mse: 0.1833 - val_loss: 0.1251 - val_mse: 0.1251\n",
      "Epoch 71/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1803 - mse: 0.1803 - val_loss: 0.1268 - val_mse: 0.1268\n",
      "Epoch 72/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1775 - mse: 0.1775 - val_loss: 0.1283 - val_mse: 0.1283\n",
      "Epoch 73/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1750 - mse: 0.1750 - val_loss: 0.1269 - val_mse: 0.1269\n",
      "Epoch 74/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1727 - mse: 0.1727 - val_loss: 0.1230 - val_mse: 0.1230\n",
      "Epoch 75/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1705 - mse: 0.1705 - val_loss: 0.1192 - val_mse: 0.1192\n",
      "Epoch 76/220\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1685 - mse: 0.1685 - val_loss: 0.1173 - val_mse: 0.1173\n",
      "Epoch 77/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1667 - mse: 0.1667 - val_loss: 0.1175 - val_mse: 0.1175\n",
      "Epoch 78/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1649 - mse: 0.1649 - val_loss: 0.1183 - val_mse: 0.1183\n",
      "Epoch 79/220\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1634 - mse: 0.1634 - val_loss: 0.1180 - val_mse: 0.1180\n",
      "Epoch 80/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1619 - mse: 0.1619 - val_loss: 0.1163 - val_mse: 0.1163\n",
      "Epoch 81/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1606 - mse: 0.1606 - val_loss: 0.1144 - val_mse: 0.1144\n",
      "Epoch 82/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1593 - mse: 0.1593 - val_loss: 0.1134 - val_mse: 0.1134\n",
      "Epoch 83/220\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1582 - mse: 0.1582 - val_loss: 0.1134 - val_mse: 0.1134\n",
      "Epoch 84/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1572 - mse: 0.1572 - val_loss: 0.1138 - val_mse: 0.1138\n",
      "Epoch 85/220\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1562 - mse: 0.1562 - val_loss: 0.1139 - val_mse: 0.1139\n",
      "Epoch 86/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1554 - mse: 0.1554 - val_loss: 0.1135 - val_mse: 0.1135\n",
      "Epoch 87/220\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1547 - mse: 0.1547 - val_loss: 0.1131 - val_mse: 0.1131\n",
      "Epoch 88/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1541 - mse: 0.1541 - val_loss: 0.1130 - val_mse: 0.1130\n",
      "Epoch 89/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1535 - mse: 0.1535 - val_loss: 0.1133 - val_mse: 0.1133\n",
      "Epoch 90/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1530 - mse: 0.1530 - val_loss: 0.1137 - val_mse: 0.1137\n",
      "Epoch 91/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1526 - mse: 0.1526 - val_loss: 0.1140 - val_mse: 0.1140\n",
      "Epoch 92/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1522 - mse: 0.1522 - val_loss: 0.1142 - val_mse: 0.1142\n",
      "Epoch 93/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1519 - mse: 0.1519 - val_loss: 0.1147 - val_mse: 0.1147\n",
      "Epoch 94/220\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1517 - mse: 0.1517 - val_loss: 0.1152 - val_mse: 0.1152\n",
      "Epoch 95/220\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1515 - mse: 0.1515 - val_loss: 0.1156 - val_mse: 0.1156\n",
      "Epoch 96/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1513 - mse: 0.1513 - val_loss: 0.1159 - val_mse: 0.1159\n",
      "Epoch 97/220\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1511 - mse: 0.1511 - val_loss: 0.1163 - val_mse: 0.1163\n",
      "Epoch 98/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1510 - mse: 0.1510 - val_loss: 0.1169 - val_mse: 0.1169\n",
      "Epoch 99/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1509 - mse: 0.1509 - val_loss: 0.1176 - val_mse: 0.1176\n",
      "Epoch 100/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1508 - mse: 0.1508 - val_loss: 0.1182 - val_mse: 0.1182\n",
      "Epoch 101/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1507 - mse: 0.1507 - val_loss: 0.1185 - val_mse: 0.1185\n",
      "Epoch 102/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1506 - mse: 0.1506 - val_loss: 0.1187 - val_mse: 0.1187\n",
      "Epoch 103/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1505 - mse: 0.1505 - val_loss: 0.1191 - val_mse: 0.1191\n",
      "Epoch 104/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1504 - mse: 0.1504 - val_loss: 0.1196 - val_mse: 0.1196\n",
      "Epoch 105/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1503 - mse: 0.1503 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 106/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1502 - mse: 0.1502 - val_loss: 0.1207 - val_mse: 0.1207\n",
      "Epoch 107/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1501 - mse: 0.1501 - val_loss: 0.1208 - val_mse: 0.1208\n",
      "Epoch 108/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1500 - mse: 0.1500 - val_loss: 0.1209 - val_mse: 0.1209\n",
      "Epoch 109/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1499 - mse: 0.1499 - val_loss: 0.1212 - val_mse: 0.1212\n",
      "Epoch 110/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1497 - mse: 0.1497 - val_loss: 0.1216 - val_mse: 0.1216\n",
      "Epoch 111/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1496 - mse: 0.1496 - val_loss: 0.1220 - val_mse: 0.1220\n",
      "Epoch 112/220\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1494 - mse: 0.1494 - val_loss: 0.1222 - val_mse: 0.1222\n",
      "Epoch 113/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1493 - mse: 0.1493 - val_loss: 0.1221 - val_mse: 0.1221\n",
      "Epoch 114/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1491 - mse: 0.1491 - val_loss: 0.1222 - val_mse: 0.1222\n",
      "Epoch 115/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1490 - mse: 0.1490 - val_loss: 0.1223 - val_mse: 0.1223\n",
      "Epoch 116/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1488 - mse: 0.1488 - val_loss: 0.1226 - val_mse: 0.1226\n",
      "Epoch 117/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1487 - mse: 0.1487 - val_loss: 0.1227 - val_mse: 0.1227\n",
      "Epoch 118/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1485 - mse: 0.1485 - val_loss: 0.1227 - val_mse: 0.1227\n",
      "Epoch 119/220\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1484 - mse: 0.1484 - val_loss: 0.1226 - val_mse: 0.1226\n",
      "Epoch 120/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1482 - mse: 0.1482 - val_loss: 0.1226 - val_mse: 0.1226\n",
      "Epoch 121/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1481 - mse: 0.1481 - val_loss: 0.1227 - val_mse: 0.1227\n",
      "Epoch 122/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1479 - mse: 0.1479 - val_loss: 0.1228 - val_mse: 0.1228\n",
      "Epoch 123/220\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1478 - mse: 0.1478 - val_loss: 0.1227 - val_mse: 0.1227\n",
      "Epoch 124/220\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1476 - mse: 0.1476 - val_loss: 0.1225 - val_mse: 0.1225\n",
      "Epoch 125/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1475 - mse: 0.1475 - val_loss: 0.1224 - val_mse: 0.1224\n",
      "Epoch 126/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1473 - mse: 0.1473 - val_loss: 0.1224 - val_mse: 0.1224\n",
      "Epoch 127/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1472 - mse: 0.1472 - val_loss: 0.1224 - val_mse: 0.1224\n",
      "Epoch 128/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1470 - mse: 0.1470 - val_loss: 0.1223 - val_mse: 0.1223\n",
      "Epoch 129/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1469 - mse: 0.1469 - val_loss: 0.1221 - val_mse: 0.1221\n",
      "Epoch 130/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1467 - mse: 0.1467 - val_loss: 0.1219 - val_mse: 0.1219\n",
      "Epoch 131/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1466 - mse: 0.1466 - val_loss: 0.1218 - val_mse: 0.1218\n",
      "Epoch 132/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1464 - mse: 0.1464 - val_loss: 0.1218 - val_mse: 0.1218\n",
      "Epoch 133/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1463 - mse: 0.1463 - val_loss: 0.1217 - val_mse: 0.1217\n",
      "Epoch 134/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1461 - mse: 0.1461 - val_loss: 0.1215 - val_mse: 0.1215\n",
      "Epoch 135/220\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1460 - mse: 0.1460 - val_loss: 0.1213 - val_mse: 0.1213\n",
      "Epoch 136/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1459 - mse: 0.1459 - val_loss: 0.1212 - val_mse: 0.1212\n",
      "Epoch 137/220\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1457 - mse: 0.1457 - val_loss: 0.1211 - val_mse: 0.1211\n",
      "Epoch 138/220\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1456 - mse: 0.1456 - val_loss: 0.1210 - val_mse: 0.1210\n",
      "Epoch 139/220\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1454 - mse: 0.1454 - val_loss: 0.1209 - val_mse: 0.1209\n",
      "Epoch 140/220\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1453 - mse: 0.1453 - val_loss: 0.1208 - val_mse: 0.1208\n",
      "Epoch 141/220\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1451 - mse: 0.1451 - val_loss: 0.1207 - val_mse: 0.1207\n",
      "Epoch 142/220\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1449 - mse: 0.1449 - val_loss: 0.1206 - val_mse: 0.1206\n",
      "Epoch 143/220\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1448 - mse: 0.1448 - val_loss: 0.1206 - val_mse: 0.1206\n",
      "Epoch 144/220\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1446 - mse: 0.1446 - val_loss: 0.1205 - val_mse: 0.1205\n",
      "Epoch 145/220\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1445 - mse: 0.1445 - val_loss: 0.1204 - val_mse: 0.1204\n",
      "Epoch 146/220\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1443 - mse: 0.1443 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 147/220\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1442 - mse: 0.1442 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 148/220\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1440 - mse: 0.1440 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 149/220\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1438 - mse: 0.1438 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 150/220\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1437 - mse: 0.1437 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 151/220\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1435 - mse: 0.1435 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 152/220\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1433 - mse: 0.1433 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 153/220\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1431 - mse: 0.1431 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 154/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1430 - mse: 0.1430 - val_loss: 0.1204 - val_mse: 0.1204\n",
      "Epoch 155/220\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1428 - mse: 0.1428 - val_loss: 0.1204 - val_mse: 0.1204\n",
      "Epoch 156/220\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1426 - mse: 0.1426 - val_loss: 0.1205 - val_mse: 0.1205\n",
      "Epoch 157/220\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1424 - mse: 0.1424 - val_loss: 0.1205 - val_mse: 0.1205\n",
      "Epoch 158/220\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1422 - mse: 0.1422 - val_loss: 0.1206 - val_mse: 0.1206\n",
      "Epoch 159/220\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1420 - mse: 0.1420 - val_loss: 0.1207 - val_mse: 0.1207\n",
      "Epoch 160/220\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1418 - mse: 0.1418 - val_loss: 0.1208 - val_mse: 0.1208\n",
      "Epoch 161/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1416 - mse: 0.1416 - val_loss: 0.1209 - val_mse: 0.1209\n",
      "Epoch 162/220\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1414 - mse: 0.1414 - val_loss: 0.1209 - val_mse: 0.1209\n",
      "Epoch 163/220\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1412 - mse: 0.1412 - val_loss: 0.1211 - val_mse: 0.1211\n",
      "Epoch 164/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1410 - mse: 0.1410 - val_loss: 0.1212 - val_mse: 0.1212\n",
      "Epoch 165/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1408 - mse: 0.1408 - val_loss: 0.1213 - val_mse: 0.1213\n",
      "Epoch 166/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1405 - mse: 0.1405 - val_loss: 0.1214 - val_mse: 0.1214\n",
      "Epoch 167/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1403 - mse: 0.1403 - val_loss: 0.1215 - val_mse: 0.1215\n",
      "Epoch 168/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1401 - mse: 0.1401 - val_loss: 0.1217 - val_mse: 0.1217\n",
      "Epoch 169/220\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1398 - mse: 0.1398 - val_loss: 0.1218 - val_mse: 0.1218\n",
      "Epoch 170/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1396 - mse: 0.1396 - val_loss: 0.1219 - val_mse: 0.1219\n",
      "Epoch 171/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1394 - mse: 0.1394 - val_loss: 0.1220 - val_mse: 0.1220\n",
      "Epoch 172/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1391 - mse: 0.1391 - val_loss: 0.1222 - val_mse: 0.1222\n",
      "Epoch 173/220\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1388 - mse: 0.1388 - val_loss: 0.1223 - val_mse: 0.1223\n",
      "Epoch 174/220\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1386 - mse: 0.1386 - val_loss: 0.1224 - val_mse: 0.1224\n",
      "Epoch 175/220\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1383 - mse: 0.1383 - val_loss: 0.1226 - val_mse: 0.1226\n",
      "Epoch 176/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1380 - mse: 0.1380 - val_loss: 0.1227 - val_mse: 0.1227\n",
      "Epoch 177/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1378 - mse: 0.1378 - val_loss: 0.1228 - val_mse: 0.1228\n",
      "Epoch 178/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1375 - mse: 0.1375 - val_loss: 0.1229 - val_mse: 0.1229\n",
      "Epoch 179/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1372 - mse: 0.1372 - val_loss: 0.1230 - val_mse: 0.1230\n",
      "Epoch 180/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1369 - mse: 0.1369 - val_loss: 0.1232 - val_mse: 0.1232\n",
      "Epoch 181/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1366 - mse: 0.1366 - val_loss: 0.1233 - val_mse: 0.1233\n",
      "Epoch 182/220\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1362 - mse: 0.1362 - val_loss: 0.1234 - val_mse: 0.1234\n",
      "Epoch 183/220\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1359 - mse: 0.1359 - val_loss: 0.1235 - val_mse: 0.1235\n",
      "Epoch 184/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1356 - mse: 0.1356 - val_loss: 0.1236 - val_mse: 0.1236\n",
      "Epoch 185/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1353 - mse: 0.1353 - val_loss: 0.1237 - val_mse: 0.1237\n",
      "Epoch 186/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1349 - mse: 0.1349 - val_loss: 0.1238 - val_mse: 0.1238\n",
      "Epoch 187/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1346 - mse: 0.1346 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 188/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1342 - mse: 0.1342 - val_loss: 0.1240 - val_mse: 0.1240\n",
      "Epoch 189/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1338 - mse: 0.1338 - val_loss: 0.1241 - val_mse: 0.1241\n",
      "Epoch 190/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1334 - mse: 0.1334 - val_loss: 0.1242 - val_mse: 0.1242\n",
      "Epoch 191/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1331 - mse: 0.1331 - val_loss: 0.1242 - val_mse: 0.1242\n",
      "Epoch 192/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1327 - mse: 0.1327 - val_loss: 0.1243 - val_mse: 0.1243\n",
      "Epoch 193/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1323 - mse: 0.1323 - val_loss: 0.1244 - val_mse: 0.1244\n",
      "Epoch 194/220\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1319 - mse: 0.1319 - val_loss: 0.1245 - val_mse: 0.1245\n",
      "Epoch 195/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1314 - mse: 0.1314 - val_loss: 0.1246 - val_mse: 0.1246\n",
      "Epoch 196/220\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1310 - mse: 0.1310 - val_loss: 0.1246 - val_mse: 0.1246\n",
      "Epoch 197/220\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1306 - mse: 0.1306 - val_loss: 0.1247 - val_mse: 0.1247\n",
      "Epoch 198/220\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1301 - mse: 0.1301 - val_loss: 0.1248 - val_mse: 0.1248\n",
      "Epoch 199/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1297 - mse: 0.1297 - val_loss: 0.1248 - val_mse: 0.1248\n",
      "Epoch 200/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1292 - mse: 0.1292 - val_loss: 0.1249 - val_mse: 0.1249\n",
      "Epoch 201/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1288 - mse: 0.1288 - val_loss: 0.1250 - val_mse: 0.1250\n",
      "Epoch 202/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1283 - mse: 0.1283 - val_loss: 0.1250 - val_mse: 0.1250\n",
      "Epoch 203/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1278 - mse: 0.1278 - val_loss: 0.1251 - val_mse: 0.1251\n",
      "Epoch 204/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1273 - mse: 0.1273 - val_loss: 0.1251 - val_mse: 0.1251\n",
      "Epoch 205/220\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1268 - mse: 0.1268 - val_loss: 0.1252 - val_mse: 0.1252\n",
      "Epoch 206/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1263 - mse: 0.1263 - val_loss: 0.1252 - val_mse: 0.1252\n",
      "Epoch 207/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1258 - mse: 0.1258 - val_loss: 0.1252 - val_mse: 0.1252\n",
      "Epoch 208/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1253 - mse: 0.1253 - val_loss: 0.1253 - val_mse: 0.1253\n",
      "Epoch 209/220\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1248 - mse: 0.1248 - val_loss: 0.1253 - val_mse: 0.1253\n",
      "Epoch 210/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1242 - mse: 0.1242 - val_loss: 0.1253 - val_mse: 0.1253\n",
      "Epoch 211/220\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1237 - mse: 0.1237 - val_loss: 0.1254 - val_mse: 0.1254\n",
      "Epoch 212/220\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1231 - mse: 0.1231 - val_loss: 0.1254 - val_mse: 0.1254\n",
      "Epoch 213/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1226 - mse: 0.1226 - val_loss: 0.1254 - val_mse: 0.1254\n",
      "Epoch 214/220\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1220 - mse: 0.1220 - val_loss: 0.1254 - val_mse: 0.1254\n",
      "Epoch 215/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1214 - mse: 0.1214 - val_loss: 0.1254 - val_mse: 0.1254\n",
      "Epoch 216/220\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1209 - mse: 0.1209 - val_loss: 0.1254 - val_mse: 0.1254\n",
      "Epoch 217/220\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1203 - mse: 0.1203 - val_loss: 0.1254 - val_mse: 0.1254\n",
      "Epoch 218/220\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1197 - mse: 0.1197 - val_loss: 0.1254 - val_mse: 0.1254\n",
      "Epoch 219/220\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1191 - mse: 0.1191 - val_loss: 0.1254 - val_mse: 0.1254\n",
      "Epoch 220/220\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1185 - mse: 0.1185 - val_loss: 0.1253 - val_mse: 0.1253\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.python.keras.layers import Dense\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "variables,results = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Nu')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Nu')\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Dense(256, activation='sigmoid', input_shape=[len(variables.keys())]),\n",
    "  # layers.Dropout(0.1),\n",
    "  layers.Dense(256, activation='sigmoid'),\n",
    "  # layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "# optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mse'])\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10)\n",
    "history = model.fit(variables.values,results.values,epochs=220,validation_split=0.3,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2170c7c4c70>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss','val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.69904613],\n",
       "       [ 0.8978878 ],\n",
       "       [-1.520575  ]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess = model.predict(test_x.values)\n",
    "guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>E</th>\n",
       "      <th>N</th>\n",
       "      <th>Nu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.845154</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.699046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.845154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0.897888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.183216</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-1.520575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A         E         N        Nu\n",
       "9  -0.845154 -1.224745  0.000000  0.699046\n",
       "10 -0.845154  0.000000 -1.224745  0.897888\n",
       "11  1.183216  1.224745  1.224745 -1.520575"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizar = test_x.copy()\n",
    "desnormalizar['Nu'] = guess\n",
    "desnormalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.        ,  1.        ,  8.        , 94.83879457],\n",
       "       [25.        ,  2.        ,  5.        , 99.5221113 ],\n",
       "       [35.        ,  3.        , 11.        , 42.56007501]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizado_teste = standardscaler.inverse_transform(desnormalizar)\n",
    "desnormalizado_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>E</th>\n",
       "      <th>N</th>\n",
       "      <th>Nu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.845154</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.845154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>0.477469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.183216</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-1.991431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A         E         N        Nu\n",
       "9  -0.845154 -1.224745  0.000000  0.847698\n",
       "10 -0.845154  0.000000 -1.224745  0.477469\n",
       "11  1.183216  1.224745  1.224745 -1.991431"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizar = test_x.copy()\n",
    "desnormalizar['Nu'] = test_y\n",
    "desnormalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.  ,  1.  ,  8.  , 98.34],\n",
       "       [25.  ,  2.  ,  5.  , 89.62],\n",
       "       [35.  ,  3.  , 11.  , 31.47]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizado_resultado = standardscaler.inverse_transform(desnormalizar)\n",
    "desnormalizado_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.9117297348696398 meansquarederror: 77.7666704711056 meanabsoluteerror: 8.16446391269438 maxerror: 11.090075013411045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9117297348696398, 77.7666704711056, 8.16446391269438, 11.090075013411045)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores(desnormalizado_resultado[:,-1],desnormalizado_teste[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"keras_model_Nu.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edbada794071f9c875b2bc5e0e869a1d746a19a0ba8801f10239845106c51c5a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
