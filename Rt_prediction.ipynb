{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>E</th>\n",
       "      <th>N</th>\n",
       "      <th>Rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A  E   N     Rt\n",
       "0   25  1  11  1.638\n",
       "1   25  1   5  2.595\n",
       "2   25  2   8  1.976\n",
       "3   25  3  11  4.164\n",
       "4   25  3   5  2.869\n",
       "5   35  1   8  1.421\n",
       "6   35  2  11  1.564\n",
       "7   35  2   5  2.101\n",
       "8   35  3   8  1.591\n",
       "9   25  1   8  1.946\n",
       "10  25  2   5  2.991\n",
       "11  35  3  11  2.896"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "database = pd.read_excel('database_TCC.xlsx')\n",
    "database = database[['A','E','N','Rt']]\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardscaler = StandardScaler()\n",
    "standardscaler.fit(database)\n",
    "data = standardscaler.transform(database)\n",
    "database = pd.DataFrame(data,columns=database.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(standardscaler, open('standard_scaler_Rt.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error,max_error\n",
    "\n",
    "def split_x_and_y(database,x,y):\n",
    "  dataset_x = database[x]\n",
    "  dataset_y = database[y]\n",
    "  return dataset_x,dataset_y\n",
    "\n",
    "\n",
    "def scores(Y_true, Y_predicted):\n",
    "  r2 = r2_score(Y_true, Y_predicted)\n",
    "  meansquarederror = mean_squared_error(Y_true, Y_predicted)\n",
    "  meanabsoluteerror = mean_absolute_error(Y_true, Y_predicted)\n",
    "  maxerror = max_error(Y_true, Y_predicted)\n",
    "\n",
    "  print('r2:',r2,'meansquarederror:',meansquarederror,'meanabsoluteerror:',meanabsoluteerror,'maxerror:',maxerror)\n",
    "  return r2,meansquarederror,meanabsoluteerror,maxerror\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3306427822593704"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# database.iloc[[0,2,4,5,7,8]]\n",
    "\n",
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Rt')\n",
    "\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Rt')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFR = RandomForestRegressor(n_estimators=5000,random_state=100)\n",
    "RFR.fit(dataset_x,dataset_y)\n",
    "RFR.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21042855, 0.33216308, 0.45740836])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFR.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(RFR, open('Random_Forest_Regressor_Rt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4359749662202925"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Rt')\n",
    "\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Rt')\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "svr = svm.SVR(kernel='sigmoid',C=3,gamma='auto')\n",
    "\n",
    "svr.fit(dataset_x,dataset_y)\n",
    "svr.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(svr, open('Support_Vector_Machine_Rt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05892459425244634"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_x,dataset_y = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Rt')\n",
    "\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Rt')\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "MLP = MLPRegressor(max_iter=1000,random_state=100)\n",
    "MLP.fit(dataset_x,dataset_y)\n",
    "MLP.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(MLP, open('MLP_Rt.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 1.3499 - mse: 1.3499 - val_loss: 0.7119 - val_mse: 0.7119\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3135 - mse: 1.3135 - val_loss: 0.6378 - val_mse: 0.6378\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2470 - mse: 1.2470 - val_loss: 0.4989 - val_mse: 0.4989\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1988 - mse: 1.1988 - val_loss: 0.4252 - val_mse: 0.4252\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1728 - mse: 1.1728 - val_loss: 0.4344 - val_mse: 0.4344\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.1408 - mse: 1.1408 - val_loss: 0.5005 - val_mse: 0.5005\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.1066 - mse: 1.1066 - val_loss: 0.5883 - val_mse: 0.5883\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0793 - mse: 1.0793 - val_loss: 0.6535 - val_mse: 0.6535\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0564 - mse: 1.0564 - val_loss: 0.6684 - val_mse: 0.6684\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0299 - mse: 1.0299 - val_loss: 0.6385 - val_mse: 0.6385\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9999 - mse: 0.9999 - val_loss: 0.5914 - val_mse: 0.5914\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9716 - mse: 0.9716 - val_loss: 0.5576 - val_mse: 0.5576\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9469 - mse: 0.9469 - val_loss: 0.5557 - val_mse: 0.5557\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9216 - mse: 0.9216 - val_loss: 0.5894 - val_mse: 0.5894\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8934 - mse: 0.8934 - val_loss: 0.6447 - val_mse: 0.6447\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8664 - mse: 0.8664 - val_loss: 0.7004 - val_mse: 0.7004\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8420 - mse: 0.8420 - val_loss: 0.7335 - val_mse: 0.7335\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8180 - mse: 0.8180 - val_loss: 0.7335 - val_mse: 0.7335\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7926 - mse: 0.7926 - val_loss: 0.7099 - val_mse: 0.7099\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7672 - mse: 0.7672 - val_loss: 0.6841 - val_mse: 0.6841\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7439 - mse: 0.7439 - val_loss: 0.6752 - val_mse: 0.6752\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7217 - mse: 0.7217 - val_loss: 0.6906 - val_mse: 0.6906\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6992 - mse: 0.6992 - val_loss: 0.7262 - val_mse: 0.7262\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6769 - mse: 0.6769 - val_loss: 0.7676 - val_mse: 0.7676\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6561 - mse: 0.6561 - val_loss: 0.7965 - val_mse: 0.7965\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6365 - mse: 0.6365 - val_loss: 0.8017 - val_mse: 0.8017\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6168 - mse: 0.6168 - val_loss: 0.7895 - val_mse: 0.7895\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5980 - mse: 0.5980 - val_loss: 0.7755 - val_mse: 0.7755\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5807 - mse: 0.5807 - val_loss: 0.7758 - val_mse: 0.7758\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5649 - mse: 0.5649 - val_loss: 0.7956 - val_mse: 0.7956\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5493 - mse: 0.5493 - val_loss: 0.8235 - val_mse: 0.8235\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5352 - mse: 0.5352 - val_loss: 0.8439 - val_mse: 0.8439\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5235 - mse: 0.5235 - val_loss: 0.8475 - val_mse: 0.8475\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5119 - mse: 0.5119 - val_loss: 0.8383 - val_mse: 0.8383\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5013 - mse: 0.5013 - val_loss: 0.8307 - val_mse: 0.8307\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4922 - mse: 0.4922 - val_loss: 0.8348 - val_mse: 0.8348\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4842 - mse: 0.4842 - val_loss: 0.8499 - val_mse: 0.8499\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4767 - mse: 0.4767 - val_loss: 0.8662 - val_mse: 0.8662\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4708 - mse: 0.4708 - val_loss: 0.8694 - val_mse: 0.8694\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4656 - mse: 0.4656 - val_loss: 0.8618 - val_mse: 0.8618\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4612 - mse: 0.4612 - val_loss: 0.8503 - val_mse: 0.8503\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4569 - mse: 0.4569 - val_loss: 0.8454 - val_mse: 0.8454\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4537 - mse: 0.4537 - val_loss: 0.8511 - val_mse: 0.8511\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4510 - mse: 0.4510 - val_loss: 0.8594 - val_mse: 0.8594\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4480 - mse: 0.4480 - val_loss: 0.8596 - val_mse: 0.8596\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4447 - mse: 0.4447 - val_loss: 0.8506 - val_mse: 0.8506\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4415 - mse: 0.4415 - val_loss: 0.8351 - val_mse: 0.8351\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4386 - mse: 0.4386 - val_loss: 0.8210 - val_mse: 0.8210\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4359 - mse: 0.4359 - val_loss: 0.8145 - val_mse: 0.8145\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4329 - mse: 0.4329 - val_loss: 0.8119 - val_mse: 0.8119\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4307 - mse: 0.4307 - val_loss: 0.8091 - val_mse: 0.8091\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4281 - mse: 0.4281 - val_loss: 0.7981 - val_mse: 0.7981\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4268 - mse: 0.4268 - val_loss: 0.7791 - val_mse: 0.7791\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4256 - mse: 0.4256 - val_loss: 0.7568 - val_mse: 0.7568\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4234 - mse: 0.4234 - val_loss: 0.7430 - val_mse: 0.7430\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4208 - mse: 0.4208 - val_loss: 0.7389 - val_mse: 0.7389\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4186 - mse: 0.4186 - val_loss: 0.7343 - val_mse: 0.7343\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4161 - mse: 0.4161 - val_loss: 0.7238 - val_mse: 0.7238\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4139 - mse: 0.4139 - val_loss: 0.7125 - val_mse: 0.7125\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4115 - mse: 0.4115 - val_loss: 0.6969 - val_mse: 0.6969\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4084 - mse: 0.4084 - val_loss: 0.6921 - val_mse: 0.6921\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4059 - mse: 0.4059 - val_loss: 0.6933 - val_mse: 0.6933\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4044 - mse: 0.4044 - val_loss: 0.6893 - val_mse: 0.6893\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4010 - mse: 0.4010 - val_loss: 0.6812 - val_mse: 0.6812\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3985 - mse: 0.3985 - val_loss: 0.6756 - val_mse: 0.6756\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3971 - mse: 0.3971 - val_loss: 0.6710 - val_mse: 0.6710\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3946 - mse: 0.3946 - val_loss: 0.6679 - val_mse: 0.6679\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3917 - mse: 0.3917 - val_loss: 0.6650 - val_mse: 0.6650\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3894 - mse: 0.3894 - val_loss: 0.6587 - val_mse: 0.6587\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3880 - mse: 0.3880 - val_loss: 0.6482 - val_mse: 0.6482\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3862 - mse: 0.3862 - val_loss: 0.6390 - val_mse: 0.6390\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3841 - mse: 0.3841 - val_loss: 0.6288 - val_mse: 0.6288\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3811 - mse: 0.3811 - val_loss: 0.6205 - val_mse: 0.6205\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3784 - mse: 0.3784 - val_loss: 0.6146 - val_mse: 0.6146\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3757 - mse: 0.3757 - val_loss: 0.6091 - val_mse: 0.6091\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3742 - mse: 0.3742 - val_loss: 0.6021 - val_mse: 0.6021\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3723 - mse: 0.3723 - val_loss: 0.5982 - val_mse: 0.5982\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3689 - mse: 0.3689 - val_loss: 0.5951 - val_mse: 0.5951\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3675 - mse: 0.3675 - val_loss: 0.5898 - val_mse: 0.5898\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3657 - mse: 0.3657 - val_loss: 0.5878 - val_mse: 0.5878\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3621 - mse: 0.3621 - val_loss: 0.5897 - val_mse: 0.5897\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3583 - mse: 0.3583 - val_loss: 0.5932 - val_mse: 0.5932\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3564 - mse: 0.3564 - val_loss: 0.5943 - val_mse: 0.5943\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3540 - mse: 0.3540 - val_loss: 0.5922 - val_mse: 0.5922\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3515 - mse: 0.3515 - val_loss: 0.5895 - val_mse: 0.5895\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3487 - mse: 0.3487 - val_loss: 0.5862 - val_mse: 0.5862\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3450 - mse: 0.3450 - val_loss: 0.5831 - val_mse: 0.5831\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3422 - mse: 0.3422 - val_loss: 0.5792 - val_mse: 0.5792\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3401 - mse: 0.3401 - val_loss: 0.5774 - val_mse: 0.5774\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3366 - mse: 0.3366 - val_loss: 0.5747 - val_mse: 0.5747\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3338 - mse: 0.3338 - val_loss: 0.5752 - val_mse: 0.5752\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3310 - mse: 0.3310 - val_loss: 0.5759 - val_mse: 0.5759\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3277 - mse: 0.3277 - val_loss: 0.5733 - val_mse: 0.5733\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3248 - mse: 0.3248 - val_loss: 0.5748 - val_mse: 0.5748\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3220 - mse: 0.3220 - val_loss: 0.5814 - val_mse: 0.5814\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3187 - mse: 0.3187 - val_loss: 0.5833 - val_mse: 0.5833\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3163 - mse: 0.3163 - val_loss: 0.5800 - val_mse: 0.5800\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3126 - mse: 0.3126 - val_loss: 0.5779 - val_mse: 0.5779\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3092 - mse: 0.3092 - val_loss: 0.5758 - val_mse: 0.5758\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3059 - mse: 0.3059 - val_loss: 0.5723 - val_mse: 0.5723\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3025 - mse: 0.3025 - val_loss: 0.5694 - val_mse: 0.5694\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2984 - mse: 0.2984 - val_loss: 0.5680 - val_mse: 0.5680\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2949 - mse: 0.2949 - val_loss: 0.5681 - val_mse: 0.5681\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2911 - mse: 0.2911 - val_loss: 0.5671 - val_mse: 0.5671\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2874 - mse: 0.2874 - val_loss: 0.5650 - val_mse: 0.5650\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2846 - mse: 0.2846 - val_loss: 0.5574 - val_mse: 0.5574\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2808 - mse: 0.2808 - val_loss: 0.5483 - val_mse: 0.5483\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2766 - mse: 0.2766 - val_loss: 0.5469 - val_mse: 0.5469\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2727 - mse: 0.2727 - val_loss: 0.5491 - val_mse: 0.5491\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2690 - mse: 0.2690 - val_loss: 0.5441 - val_mse: 0.5441\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2655 - mse: 0.2655 - val_loss: 0.5382 - val_mse: 0.5382\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.5357 - val_mse: 0.5357\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2580 - mse: 0.2580 - val_loss: 0.5409 - val_mse: 0.5409\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2540 - mse: 0.2540 - val_loss: 0.5410 - val_mse: 0.5410\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2497 - mse: 0.2497 - val_loss: 0.5345 - val_mse: 0.5345\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2454 - mse: 0.2454 - val_loss: 0.5276 - val_mse: 0.5276\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2421 - mse: 0.2421 - val_loss: 0.5280 - val_mse: 0.5280\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2385 - mse: 0.2385 - val_loss: 0.5293 - val_mse: 0.5293\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2341 - mse: 0.2341 - val_loss: 0.5274 - val_mse: 0.5274\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2297 - mse: 0.2297 - val_loss: 0.5194 - val_mse: 0.5194\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2257 - mse: 0.2257 - val_loss: 0.5129 - val_mse: 0.5129\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2218 - mse: 0.2218 - val_loss: 0.5089 - val_mse: 0.5089\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2183 - mse: 0.2183 - val_loss: 0.5070 - val_mse: 0.5070\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2133 - mse: 0.2133 - val_loss: 0.5061 - val_mse: 0.5061\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2091 - mse: 0.2091 - val_loss: 0.5001 - val_mse: 0.5001\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2044 - mse: 0.2044 - val_loss: 0.4899 - val_mse: 0.4899\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1995 - mse: 0.1995 - val_loss: 0.4828 - val_mse: 0.4828\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1957 - mse: 0.1957 - val_loss: 0.4762 - val_mse: 0.4762\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1920 - mse: 0.1920 - val_loss: 0.4699 - val_mse: 0.4699\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1873 - mse: 0.1873 - val_loss: 0.4671 - val_mse: 0.4671\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1823 - mse: 0.1823 - val_loss: 0.4603 - val_mse: 0.4603\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1771 - mse: 0.1771 - val_loss: 0.4587 - val_mse: 0.4587\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1727 - mse: 0.1727 - val_loss: 0.4564 - val_mse: 0.4564\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1698 - mse: 0.1698 - val_loss: 0.4517 - val_mse: 0.4517\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1660 - mse: 0.1660 - val_loss: 0.4467 - val_mse: 0.4467\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1617 - mse: 0.1617 - val_loss: 0.4493 - val_mse: 0.4493\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1576 - mse: 0.1576 - val_loss: 0.4553 - val_mse: 0.4553\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1544 - mse: 0.1544 - val_loss: 0.4526 - val_mse: 0.4526\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1497 - mse: 0.1497 - val_loss: 0.4406 - val_mse: 0.4406\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1451 - mse: 0.1451 - val_loss: 0.4269 - val_mse: 0.4269\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1402 - mse: 0.1402 - val_loss: 0.4226 - val_mse: 0.4226\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1373 - mse: 0.1373 - val_loss: 0.4205 - val_mse: 0.4205\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1337 - mse: 0.1337 - val_loss: 0.4184 - val_mse: 0.4184\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1299 - mse: 0.1299 - val_loss: 0.4154 - val_mse: 0.4154\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1261 - mse: 0.1261 - val_loss: 0.4141 - val_mse: 0.4141\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1218 - mse: 0.1218 - val_loss: 0.4104 - val_mse: 0.4104\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1182 - mse: 0.1182 - val_loss: 0.4082 - val_mse: 0.4082\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1143 - mse: 0.1143 - val_loss: 0.4037 - val_mse: 0.4037\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1107 - mse: 0.1107 - val_loss: 0.3985 - val_mse: 0.3985\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1069 - mse: 0.1069 - val_loss: 0.3929 - val_mse: 0.3929\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1032 - mse: 0.1032 - val_loss: 0.3913 - val_mse: 0.3913\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.3920 - val_mse: 0.3920\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0961 - mse: 0.0961 - val_loss: 0.3938 - val_mse: 0.3938\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0925 - mse: 0.0925 - val_loss: 0.3919 - val_mse: 0.3919\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0893 - mse: 0.0893 - val_loss: 0.3810 - val_mse: 0.3810\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0869 - mse: 0.0869 - val_loss: 0.3706 - val_mse: 0.3706\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0847 - mse: 0.0847 - val_loss: 0.3672 - val_mse: 0.3672\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0816 - mse: 0.0816 - val_loss: 0.3657 - val_mse: 0.3657\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0782 - mse: 0.0782 - val_loss: 0.3600 - val_mse: 0.3600\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0752 - mse: 0.0752 - val_loss: 0.3533 - val_mse: 0.3533\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0724 - mse: 0.0724 - val_loss: 0.3503 - val_mse: 0.3503\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0703 - mse: 0.0703 - val_loss: 0.3518 - val_mse: 0.3518\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0681 - mse: 0.0681 - val_loss: 0.3466 - val_mse: 0.3466\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0654 - mse: 0.0654 - val_loss: 0.3407 - val_mse: 0.3407\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0626 - mse: 0.0626 - val_loss: 0.3362 - val_mse: 0.3362\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0608 - mse: 0.0608 - val_loss: 0.3306 - val_mse: 0.3306\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0589 - mse: 0.0589 - val_loss: 0.3232 - val_mse: 0.3232\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0569 - mse: 0.0569 - val_loss: 0.3209 - val_mse: 0.3209\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0547 - mse: 0.0547 - val_loss: 0.3232 - val_mse: 0.3232\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0527 - mse: 0.0527 - val_loss: 0.3243 - val_mse: 0.3243\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0511 - mse: 0.0511 - val_loss: 0.3192 - val_mse: 0.3192\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0494 - mse: 0.0494 - val_loss: 0.3151 - val_mse: 0.3151\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0477 - mse: 0.0477 - val_loss: 0.3141 - val_mse: 0.3141\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0459 - mse: 0.0459 - val_loss: 0.3160 - val_mse: 0.3160\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.3125 - val_mse: 0.3125\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.3026 - val_mse: 0.3026\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.2981 - val_mse: 0.2981\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.2991 - val_mse: 0.2991\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.3018 - val_mse: 0.3018\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.3010 - val_mse: 0.3010\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.2956 - val_mse: 0.2956\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.2924 - val_mse: 0.2924\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.2900 - val_mse: 0.2900\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0323 - mse: 0.0323 - val_loss: 0.2912 - val_mse: 0.2912\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.2875 - val_mse: 0.2875\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.2855 - val_mse: 0.2855\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.2878 - val_mse: 0.2878\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0286 - mse: 0.0286 - val_loss: 0.2879 - val_mse: 0.2879\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0277 - mse: 0.0277 - val_loss: 0.2838 - val_mse: 0.2838\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0265 - mse: 0.0265 - val_loss: 0.2818 - val_mse: 0.2818\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.2831 - val_mse: 0.2831\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.2811 - val_mse: 0.2811\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.2774 - val_mse: 0.2774\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.2766 - val_mse: 0.2766\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.2772 - val_mse: 0.2772\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.2764 - val_mse: 0.2764\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.2729 - val_mse: 0.2729\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0201 - mse: 0.0201 - val_loss: 0.2713 - val_mse: 0.2713\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0194 - mse: 0.0194 - val_loss: 0.2717 - val_mse: 0.2717\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.2716 - val_mse: 0.2716\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.2701 - val_mse: 0.2701\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.2695 - val_mse: 0.2695\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.2706 - val_mse: 0.2706\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.2682 - val_mse: 0.2682\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.2671 - val_mse: 0.2671\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.2658 - val_mse: 0.2658\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.2610 - val_mse: 0.2610\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.2538 - val_mse: 0.2538\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.2495 - val_mse: 0.2495\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.2497 - val_mse: 0.2497\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.2500 - val_mse: 0.2500\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.2480 - val_mse: 0.2480\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.2452 - val_mse: 0.2452\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.2434 - val_mse: 0.2434\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.2431 - val_mse: 0.2431\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.2394 - val_mse: 0.2394\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.2349 - val_mse: 0.2349\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.2336 - val_mse: 0.2336\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.2352 - val_mse: 0.2352\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.2333 - val_mse: 0.2333\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.2295 - val_mse: 0.2295\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.2284 - val_mse: 0.2284\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.2291 - val_mse: 0.2291\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.2258 - val_mse: 0.2258\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.2231 - val_mse: 0.2231\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.2219 - val_mse: 0.2219\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.2214 - val_mse: 0.2214\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.2187 - val_mse: 0.2187\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.2163 - val_mse: 0.2163\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.2154 - val_mse: 0.2154\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.2172 - val_mse: 0.2172\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.2154 - val_mse: 0.2154\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.2122 - val_mse: 0.2122\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.2129 - val_mse: 0.2129\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.2134 - val_mse: 0.2134\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.2085 - val_mse: 0.2085\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.2039 - val_mse: 0.2039\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.2068 - val_mse: 0.2068\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.2088 - val_mse: 0.2088\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.2045 - val_mse: 0.2045\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.2005 - val_mse: 0.2005\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.2032 - val_mse: 0.2032\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.2044 - val_mse: 0.2044\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.1991 - val_mse: 0.1991\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.1957 - val_mse: 0.1957\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.1984 - val_mse: 0.1984\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.1986 - val_mse: 0.1986\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.1925 - val_mse: 0.1925\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.1906 - val_mse: 0.1906\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.1922 - val_mse: 0.1922\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.1907 - val_mse: 0.1907\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.1878 - val_mse: 0.1878\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.1874 - val_mse: 0.1874\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.1873 - val_mse: 0.1873\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.1860 - val_mse: 0.1860\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.1833 - val_mse: 0.1833\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.1832 - val_mse: 0.1832\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.1834 - val_mse: 0.1834\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.1810 - val_mse: 0.1810\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.1799 - val_mse: 0.1799\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.1802 - val_mse: 0.1802\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.1784 - val_mse: 0.1784\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.1763 - val_mse: 0.1763\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.1750 - val_mse: 0.1750\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.1757 - val_mse: 0.1757\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.1759 - val_mse: 0.1759\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1746 - val_mse: 0.1746\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.1738 - val_mse: 0.1738\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.1727 - val_mse: 0.1727\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.1717 - val_mse: 0.1717\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.1710 - val_mse: 0.1710\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.1702 - val_mse: 0.1702\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.1694 - val_mse: 0.1694\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.1689 - val_mse: 0.1689\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.1686 - val_mse: 0.1686\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.1667 - val_mse: 0.1667\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1669 - val_mse: 0.1669\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1666 - val_mse: 0.1666\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.1639 - val_mse: 0.1639\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.1632 - val_mse: 0.1632\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.1647 - val_mse: 0.1647\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.1648 - val_mse: 0.1648\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1626 - val_mse: 0.1626\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1623 - val_mse: 0.1623\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1628 - val_mse: 0.1628\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.1611 - val_mse: 0.1611\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.1594 - val_mse: 0.1594\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.1597 - val_mse: 0.1597\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.1605 - val_mse: 0.1605\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.1586 - val_mse: 0.1586\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.1571 - val_mse: 0.1571\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.1583 - val_mse: 0.1583\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.1580 - val_mse: 0.1580\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.1561 - val_mse: 0.1561\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.1565 - val_mse: 0.1565\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.1567 - val_mse: 0.1567\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.1549 - val_mse: 0.1549\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.1543 - val_mse: 0.1543\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.1554 - val_mse: 0.1554\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.1538 - val_mse: 0.1538\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.python.keras.layers import Dense\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "variables,results = split_x_and_y(database.iloc[[0,1,2,3,4,5,6,7,8]],['A','E','N'],'Rt')\n",
    "test_x,test_y = split_x_and_y(database.iloc[[9,10,11]],['A','E','N'],'Rt')\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Dense(64, activation='sigmoid', input_shape=[len(variables.keys())]),\n",
    "  # layers.Dropout(0.5),\n",
    "  # layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  # layers.Dense(32, activation='relu'),\n",
    "  # layers.Dense(64, activation='relu'),\n",
    "  # layers.Dropout(0.2),\n",
    "  # layers.Dense(128, activation='relu'),\n",
    "  # layers.Dense(64, activation='sigmoid'),\n",
    "  # layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "# optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mse'])\n",
    "\n",
    "history = model.fit(variables.values,results.values,epochs=300,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c1db1618b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss','val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[25.        ,  1.        ,  8.        ,  1.94816361],\n",
       "       [25.        ,  2.        ,  5.        ,  2.70565477],\n",
       "       [35.        ,  3.        , 11.        ,  2.84445748]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess = model.predict(test_x.values)\n",
    "desnormalizar = test_x.copy()\n",
    "desnormalizar['Rt'] = guess\n",
    "desnormalizado_teste = standardscaler.inverse_transform(desnormalizar)\n",
    "desnormalizado_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.   ,  1.   ,  8.   ,  1.946],\n",
       "       [25.   ,  2.   ,  5.   ,  2.991],\n",
       "       [35.   ,  3.   , 11.   ,  2.896]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desnormalizar = test_x.copy()\n",
    "desnormalizar['Rt'] = test_y\n",
    "desnormalizado_resultado = standardscaler.inverse_transform(desnormalizar)\n",
    "desnormalizado_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.8740986573441387 meansquarederror: 0.02802773723090565 meanabsoluteerror: 0.11301711868926027 maxerror: 0.2853452288352436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8740986573441387,\n",
       " 0.02802773723090565,\n",
       " 0.11301711868926027,\n",
       " 0.2853452288352436)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores(desnormalizado_resultado[:,-1],desnormalizado_teste[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"keras_model_Rt.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edbada794071f9c875b2bc5e0e869a1d746a19a0ba8801f10239845106c51c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
